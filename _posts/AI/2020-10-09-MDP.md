---


title: "강화학습 - MDP"
parent: AI
---

- ## [인공지능] Markov Decision Process, MDP

  순차적 행동 결정 문제에 대해 정의하는 방법.

  ### **MDP의 구성 요소**

  - **상태** : 시간 t 에서 상태 s 일 때, 확률변수 S<sub>t</sub> = s 라고 정의한다.

  - **행동** : 시간 t 에서 행동 a 일 때, 확률변수 A<sub>t</sub> = a 라고 정의한다.

  - **보상 함수** :   S<sub>t</sub> = s 이고 A<sub>t</sub> = a 일 때, 에이전트가 받을 보상의 기댓값(E)

    (정확한 값이 아니라 예상하는 수치, 일종의 평균으로 구함)

    
  $$
    R_s^a = E[R_{t+1} | S_t = s, A_t = a]
  $$
  
    
  
    에이전트에 주어진 환경에 따라 같은 상태, 같은 행동에서도 다른 보상을 줄 수 있기 때문에 기댓값으로 표현한다. 보상이 t+1 인 이유는 상태 s 에서 행동 a를 하는 시간인 t 의 바로 다음 t+1 에 환경이 에이전트에게 보상을 알려주기 때문이다.
  
  - **상태 변환 확률** (State Transition Probability) : 상태 s 에서 행동 a를 했을 때, 상태 s`로 변화할 확률을 나타낸다.
  
    $$
    P_{ss'}^a = P[S_{t+1} = s'| S_t = s, A_t = a]
    $$
  
  
    에이전트가 알지 못하는 환경의 일부로써 환경의 모델(model)이라고도 불린다.
  
    
  
  - **감가율** (Discount Factor) : 나중에 받을 보상의 가치를 줄이는 비율 γ 로 표기하며
  
    0 ~ 1사이의 값을 가진다.