---

layout: single

title: "강화학습 - Reinforcement Learning(4)"
categories:
  - AI
tags:
  - [Q-Learning, MonteCarlo, TemporalDifference, SALSA, e-Greedy, Prediction, Control]
---

- ## [인공지능] 큐 러닝 (Q-Learning)과 전통적 강화학습

  

  #### 강화학습의 예측과 제어

   강화학습은 환경의 모델없이 환경과의 상호 작용을 통해 최적 정책을 학습한다. 상호작용을 통해 정책에 대한 참 가치함수를 학습하는 것을 **예측**이라고 하며 예측과 함께 정책을 발전 시켜 최적 정책을 학습하는 것을 **제어**라고 한다.

  

  #### 몬테카를로 근사(Monte-Carlo Approximation)

   무한히 많은 **무작위 샘플**을 통해 무언가를 시도함으로써 값을 추정하는 것.

  >  흰 도화지에 공을 그린후 무작위 점을 찍은 후 점의 갯수를 세면 도화지의 넓이를 통해 공의 넓이를 알 수 있게된다.

  

  #### 몬테카를로 예측(Monte-Carlo Prediction)

   ***i*** 번째 에피소드에서 상태 ***s*** 를 샘플링하고 현재 정책 ***π*** 를 따라 행동하며 **반환값**(감가율이 적용된 현재~미래 보상의 합) ***G(s)<sub>i</sub>*** 를 얻어내고 모든 에피소드의 반환값을 평균 내어 ***G(s)*** 를 얻어내고 이 반환값을 목표로 가치함수를 업데이트한다. 업데이트를 계속하면 참 가치함수를 얻을 수 있다.

   결국 ***V(s)*** 는 업데이트하려는 크기를 나타내는 **스탭사이즈** ***a*** 만큼 계속 업데이트 된다. 다음은 몬테카를로 예측을 통해 가치함수가 업데이트 되는 과정을 수식으로 나타낸 것이다.

  
  $$
  V(s) ← V(s) + a(G(s) - V(s))
  $$
  
  
   몬테카를로 예측은 에피소드가 끝날때까지 기다려야 하는 단점이 있다.
  
  
  
  #### 시간차 예측(Temporal-Difference Prediction)
  
   에피소드가 아닌 매 타임스텝마다 가치함수를 업데이트하는 방법. 상태 ***s*** 를 샘플링하고 현재 상태 ***S<sub>t</sub>*** 에서 다음 상태  ***S<sub>t+1</sub>*** 로 넘어가면서 받은 보상 ***R*** 과 ***V(S<sub>t+1</sub>)*** 를 목표로 가치함수를 업데이트한다. 업데이트 목표가 정확하지 않은 상황에서 업데이트 하는 것. 업데이트를 계속하면 몬테카를로 예측보다 효율적이고 빠른 시간 안에 참 가치함수를 얻어낸다
  
   결국 ***V(s)<sub>t</sub>*** 는 스탭사이즈  ***a*** 만큼 한 타임스탭마다 계속 업데이트된다. 다음은 시간차 예측을 통해 가치함수가 업데이트 되는 과정을 수식으로 나타낸 것이다.
  
  
  $$
  V(s_t) ← V(s_t) + a(R + γV(s_{t+1})-V(S_t))
  $$
  
  
  #### 살사(SALSA) 또는 시간차 제어(Temporal-Difference Control)
  
   가치 함수를 구하는 **정책 평가**와 구해진 가치 함수를 통해 정책을 개선시키는 **정책 발전**을 한 번씩 번갈아 수행하는 정책 이터레이션을 **GPI(Generalized Policy Iteration)** 라고 한다. 시간차 방법에서는 타임 스탭마다 현재 상태에 대해서만 업데이트 하므로 모든 상태에 대해 정책 발전을 할 수 없다. 
  
   벨만 최적 방정식을 사용하는 가치 이터레이션은 시간차 방법과 같이 정책을 따로 두지 않는다. 가치 이터레이션의 가장 큰 가치를 가지는 행동을 선택하는 **탐욕 정책**을 시간차 방법에 적용하는 것을 **시간차 제어**라고 한다.
  
   시간차 방법에 가치 함수를 찾기위한 탐욕 정책을 사용하면 환경의 상태 변환 확률을 알아야 하므로 환경의 모델이 필요없는 큐함수를 찾기위한 탐욕 정책을 사용해야한다.
  
   샘플 **S<sub>t</sub>** 에서 큐함수 탐욕 정책을 통해  **A<sub>t</sub>** 를 찾아내 수행하여  **R<sub>t+1</sub>**와  **S<sub>t+1</sub>**을 얻어내고 탐욕정책을 통해  **A<sub>t+1</sub>** 까지 찾아내어 **S<sub>t</sub>, A<sub>t</sub>, R<sub>t+1</sub>,  S<sub>t+1</sub>,  A<sub>t+1</sub>**  샘플을 만들고 이를 사용하여 다음과 같은 수식을 통해 큐함수를 업데이트 한다.
  
  
  $$
  Q(S_t, A_t)←Q(S_t,A_t)+a(ㄱ + γQ(S_{t+1},A_{t+1}) - Q(S_t, A_t))
  $$
  
  
   샘플  **S<sub>t</sub>, A<sub>t</sub>, R<sub>t+1</sub>,  S<sub>t+1</sub>,  A<sub>t+1</sub>**  을 이용하기 때문에 시간차 제어를 다른말로 **살사(SARSA)**라고 부른다.
  
  
  
  ####  ε-탐욕 ( ε-Greedy) 정책
  
   초기의 에이전트에게 탐욕 정책은 잘못된 학습으로 빠지게 할 가능성이 크므로  **ε** 만큼의 확률로 탐욕적이지 않은 행동을 선택하는 **탐험(Exploration)**을 시키는 정책. 충분한 탐험은 최적 정책을 학습하기 위해 필요하다.
  
   살사와 큐러닝에서는 **ε** 의 값이 일정한 탐욕 정책을 사용하므로 최적 큐함수를 찾고도 계속 탐험한다는 한계가 있다. **ε** 의 값을 점점 감소시키는 방법도 사용한다.
  
  
  
  #### 큐러닝(Q-Learning)
  
   살사는 자신이 행동하는 대로 학습하는 시간차 제어인 **온폴리시 시간차 제어(On-Policy Temporal-Difference Control)**이므로 ε-Greedy 정책을 그대로 사용할 경우 다음과 같은 문제가 발생할 수 있다.
  
  - 실제로는 ***Q(s, a)*** 가 가장 높은 값을 주어야하는데 탐험으로 인해 ***Q(s', a')***가 마이너스 보상을 얻게되면 ***Q(s, a)*** 또한 값이 낮아지는 경우가 있다.
  
    
  
   이러한 딜레마를 해결하기 위해 **오프폴리시 시간차 제어(Off-Policy Temporal-Difference Control)** 다른말로 **큐러닝**이라 불리우는 해법을 사용하게된다.
  
    큐러닝은 오프폴리시 말그대로 행동하는 정책과 학습하는 정책을 따로 분리한다. 분리하는 방법은 다음과 같다.
  
  > 1. 상태 ***s*** 에서 ε-Greedy 정책에 따라 행동 ***a*** 를 수행하고 보상 ***r*** 을 받는다.
  >
  > 2. 다음 상태 ***s'*** 에서 Greedy(ε 가 빠진) 정책에 따라 가장 높은 큐함수  ***Q(s', a')*** 를 선택해 ***Q(s, a)*** 를 업데이트한다.
  
  
  
   오프폴리시에서는 다음 상태 ***s'*** 에서 하는 행동이 무엇이든 간에 상관 없도록 행동과 학습이 분리된다. 이를 수식으로 나타내면 다음과 같다.
  
  
  $$
  Q(S_t, A_t)←Q(S_t,A_t)+a(R_{t+1} + γmaxQ(S_{t+1},a') - Q(S_t, A_t))
  $$
  
  
   큐러닝 수식에 기댓값을 나타내는 ***E*** 를 붙이면 벨만 최적 방정식이 SALSA에 ***E*** 를 붙이면 벨만 기대 방정식으로 나타낼 수 있다.
  
  
  
   큐러닝은 살사의 탐험과 최적 정책을 결정하는 딜레마(**탐색-이용 딜레마, Exploration-exploitation dilemma**)를 정책을 분리함으로써 해결하였다. 다른 오프폴리시 강화학습과 달리 큐함수가 간단하기 때문에 강화학습 알고리즘의 토대로 사용되었다.
